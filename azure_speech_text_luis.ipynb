{"cells":[{"cell_type":"markdown","source":["1. Add the following Python Libraries to the Databricks Workspace\n    <code>\n    Click on \"Workspace\" > \"Shared\" > \"Create Library\" > \"Upload Python Egg or PyPi\" <br>\n     azure-cognitiveservices-language-luis <br>\n     azure-cognitiveservices-language-textanalytics <br>\n     SpeechRecognition <br>\n    </code>\n    Select \"Automatically attach to all clusters\"\n2. Initialize a Custom Vision Service by signing into https://customvision.ai/<br>\n    Click on the \"Settings\" Icon and copy the TRAINING KEY and PREDICTION KEY <br>"],"metadata":{}},{"cell_type":"markdown","source":["## Using the Text Analyics Cognitive Service\nThe previous examples demonstrate some ways to write code and analyze text, and they serve to illustrate that text analytics involves applying statistical techniques to text data in order to discern semantic meaning. This is a common theme in many AI solutions.\n\nMicrosoft Cognitive Services includes a Text Analytics service that encapsulates much more sophisticated techniques for ascertaining meaning from text."],"metadata":{}},{"cell_type":"markdown","source":["### Create a Text Analytics Service\nTo see this in action, you need to provision a Text Analytics service in your Azure subscription. Follow these steps to do that:\n\n1. Open another browser tab and navigate to https://portal.azure.com.\n2. Sign in using your Microsoft account.\n3. Click **+ New**, and in the **AI + Cognitive Services** category, click **See all**.\n4. In the list of cognitive services, click **Text Analytics API**.\n5. In the **Text Analytics API** blade, click **Create**.\n6. In the **Create** blade, enter the following details, and then click **Create**\n  * **Name**: A unique name for your service.\n  * **Subscription**: Your Azure subscription.\n  * **Location**: Choose the Azure datacenter location where you want to host your service.\n  * **Pricing tier**: Choose the F0 pricing tier.\n  * **Resource Group**: Choose the existing resource group you created in the previous lab (or create a new one if you didn't complete the previous lab)\n  * Read the notice about the use of your data, and select the checkbox.\n7. Wait for the service to be created.\n8. When deployment is complete, click **All Resources** and then click your Text Analytics service to open its blade.\n9. In the blade for your Text Analytics service, note the **Endpoint** URL. Then assign the base URI (*location*.api.cognitive.microsoft.com) for your service to the **textAnalyticsURI** variable in the cell below.\n10. In the blade for your Text Analytics service, click **Keys** and then copy **Key 1** to the clipboard and paste it into the **textKey** variable assignment value in the cell below. \n11. Run the cell below to assign the variables."],"metadata":{}},{"cell_type":"code","source":["textAnalyticsURI = 'eastus2.api.cognitive.microsoft.com'\ntextKey = '<INSERT TEXT ANALYTICS API KEY HERE>'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["### Call the Text Analytics Service to Determine Key Phrases in the Documents\nOne of the methods provided by the Text Analytics service is the ability to extract a list of key phrases from text documents, which give an insight into the core topics discussed in the document.\n\nRun the following cell to call the **keyPhrases** method of the Text Analytics service and extract the key phrases for the text documents you have loaded so far in this notebook."],"metadata":{}},{"cell_type":"code","source":["import http.client, urllib.request, urllib.parse, urllib.error, base64, json, urllib\n\n# Define the request headers.\nheaders = {\n    'Content-Type': 'application/json',\n    'Ocp-Apim-Subscription-Key': textKey,\n    'Accept': 'application/json'\n}\n\n# Define the parameters\nparams = urllib.parse.urlencode({\n})\n\n# Define the request body\nbody = {\n  \"documents\": [\n    {\n      \"language\": \"en\",\n      \"id\": \"1\",\n      \"text\": \"The company uses machine learning to analyze data collected from behavior on its website, and other information shared voluntarily. TrueAccord says it does not buy any personal, financial, or demographic data, including credit scores, does not use affinity data, and does not 'creep crawl the web.' But it does know how much a debtor owes, to whom, and how far behind the person is on the payments. Over time, the company believes this data will help it predict preferences, like whether customers prefer text versus email, days and times to send messages, and even tone of voice, such as empathetic, friendly, or inspirational, but never aggressive.\"\n    },\n    {\n          \"language\": \"en\",\n          \"id\": \"2\",\n          \"text\": \"The breakthrough effort was backed, in part, by a startup called Exonics, which was cofounded in 2017 by Olson and patient advocacy group CureDuchenne. Headquartered in Cambridge, Exonics has licensed the gene editing technology developed by Olson's lab and is moving it toward human trials, with the hopes of one day commercializing treatments. The young biotech company got its footing with $2 million from CureDuchenne's venture arm, and it has since raised more than $40 million from The Column Group.\"\n    }\n  ]\n}\n\ntry:\n    # Execute the REST API call and get the response.\n    conn = http.client.HTTPSConnection(textAnalyticsURI)\n    conn.request(\"POST\", \"/text/analytics/v2.0/keyPhrases?%s\" % params, str(body), headers)\n    response = conn.getresponse()\n    data = response.read().decode(\"UTF-8\")\n\n    # 'data' contains the JSON data. The following formats the JSON data for display.\n    parsed = json.loads(data)\n    for document in parsed['documents']:\n        print(\"Document \" + document[\"id\"] + \" key phrases:\")\n        for phrase in document['keyPhrases']:\n            print(\"  \" + phrase)\n        print(\"---------------------------\")\n    conn.close()\n\nexcept Exception as e:\n    print('Error:')\n    print(e)"],"metadata":{"scrolled":true},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Document 1 key phrases:\n  demographic data\n  affinity data\n  company\n  text\n  days\n  credit scores\n  messages\n  customers\n  times\n  email\n  tone of voice\n  website\n  preferences\n  creep\n  behavior\n  machine\n  person\n  payments\n  information\n  TrueAccord\n  debtor\n---------------------------\nDocument 2 key phrases:\n  Exonics\n  Column Group\n  patient advocacy group CureDuchenne\n  gene editing technology\n  hopes\n  footing\n  Olson&apos;s lab\n  day commercializing treatments\n  young biotech company\n  CureDuchenne&apos;s venture arm\n  human trials\n  Cambridge\n  breakthrough effort\n  startup\n---------------------------\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["From these key phrases, it's reasonably clear that the first document is about freedom and nationhood, while the second is about software services for AI.\n\n### Perform Sentiment Analysis\nAnother common AI requirement is to determine the sentiment associated with some text. For example, you might analyze tweets that include your organization's twitter handle to determine if they are positive or negative.\n\nRun the cell below to use the **sentiment** method of the Text Analytics service to discern the sentiment of two sentences."],"metadata":{}},{"cell_type":"code","source":["body = {\n  \"documents\": [\n    {\n      \"language\": \"en\",\n      \"id\": \"1\",\n      \"text\": \"Wow! cognitive services are fantastic.\"\n    },\n    {\n      \"language\": \"en\",\n      \"id\": \"2\",\n      \"text\": \"I hate it when computers don't understand me.\"\n    }\n  ]\n}\n\ntry:\n    conn = http.client.HTTPSConnection(textAnalyticsURI)\n    conn.request(\"POST\", \"/text/analytics/v2.0/sentiment?%s\" % params, str(body), headers)\n    response = conn.getresponse()\n    data = response.read().decode(\"UTF-8\")\n    parsed = json.loads(data)\n    for document in parsed['documents']:\n        sentiment = \"negative\"\n        if document[\"score\"] >= 0.5:\n            sentiment = \"positive\"\n        print(\"Document:\" + document[\"id\"] + \" = \" + sentiment)\n    conn.close()\n    \nexcept Exception as e:\n    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Document:1 = positive\nDocument:2 = negative\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["## Working with Speech\nSo far, we've seen how analyze text; but increasingly AI systems enable humans to communicate with software services through speech recognition.\n\n### Create a Bing Speech Service\nThe Microsoft Cognitive Services include the Bing Speech service, that can interpret spoken input from a microphone or audio file. Follow these steps to provision the Bing speech service:\n\n1. Open another browser tab and navigate to https://portal.azure.com.\n2. Sign in using your Microsoft account.\n3. Click **+ New**, and in the **AI + Cognitive Services** category, click **See all**.\n4. In the list of cognitive services, click **Bing Speech API**.\n5. In the **Bing Speech API** blade, click **Create**.\n6. In the **Create** blade, enter the following details, and then click **Create**\n  * **Name**: A unique name for your service.\n  * **Subscription**: Your Azure subscription.\n  * **Pricing tier**: Choose the F0 pricing tier.\n  * **Resource Group**: Choose the existing resource group you used previously.\n  * Read the notice about the use of your data, and select the checkbox.\n7. Wait for the service to be created.\n8. When deployment is complete, click **All Resources** and then click your Bing Speech service to open its blade.\n10. In the blade for your Bing Speech service, click **Keys** and then copy **Key 1** to the clipboard and paste it into the **textKey** variable assignment value in the cell below."],"metadata":{"collapsed":true}},{"cell_type":"code","source":["speechKey = '<INSERT BING SPEECH API KEY HERE>'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["### Install the SpeechRecognition Package\nYou can call the Bing Speech API by sending HTTP requests containing a JSON payload as you did for the Text Analytics API. However, the Bing Speech API is supported in a pre-built Python package named **SpeechRecognition**, which makes it easier to use. You can find out more about this package at https://pypi.python.org/pypi/SpeechRecognition.\n\nWe have attached the **SpeechRecognition** package to the cluster running this notebook\n\nWe'll use the following sample file for testing:<br>\nhttps://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/RainSpain.wav <br>\nRight-click on above links and \"Save Link As..\" to download the audio files and listen to it"],"metadata":{}},{"cell_type":"code","source":["%sh\nmkdir /dbfs/FileStore/media\ncurl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/RainSpain.wav -o /dbfs/FileStore/media/RainSpain.wav\nls -lt /dbfs/FileStore/media"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100  648k  100  648k    0     0  1091k      0 --:--:-- --:--:-- --:--:-- 1092k\ntotal 0\n-rw-r--r-- 1 root root 663648 Sep  4 18:45 RainSpain.wav\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["### Call the Bing Speech Service to Transcribe the Audio\nNow that we know whats in the audio file, let's see how the Bing Speech API does!"],"metadata":{}},{"cell_type":"code","source":["import speech_recognition as sr\n\naudioFile = '/dbfs/FileStore/media/RainSpain.wav'\n\n# Read the audio file\nr = sr.Recognizer(audioFile)\nwith sr.AudioFile() as source:\n    audio = r.record(source)\n    \n# transcribe speech using the Bing Speech API\ntry:\n    transcription = r.recognize_bing(audio, key=speechKey)\n    print(\"Bing Transcription:\")\n    print('\"' + transcription + '\"')\n    \nexcept sr.UnknownValueError:\n    print(\"The audio was unclear\")\nexcept sr.RequestError as e:\n    print (e)\n    print(\"Something went wrong :-(; {0}\".format(e))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Here&apos;s what I heard:\n&quot;The rain in Spain stays mainly in the plain.&quot;\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["## Using the Language Understanding Intelligence Service (LUIS)\nIncreasingly, we expect computers to be able to use AI in order to understand spoken or typed commands in natural language. For example, we want to be able to say \"switch on the light\" or \"put the light on\", and have an AI-powered device understand the command and take appropriate action.\n\n### Provision the Language Understanding Intelligence Service (LUIS)\nThe Microsoft cognitive services include the Language Understanding Intelligence Service (LUIS), which enables you to define *intents* that are applied to *entities* based on *utterances*.\n\nTo get started with LUIS, follow these steps to provision the service in your Azure subscription:\n1. Open another browser tab and navigate to https://portal.azure.com.\n2. Sign in using your Microsoft account.\n3. Click **+ New**, and in the **AI + Cognitive Services** category, click **See all**.\n4. In the list of cognitive services, click **Language Understanding**.\n5. In the **Language Understanding** blade, click **Create**.\n6. In the **Create** blade, enter the following details, and then click **Create**\n  * **Name**: A unique name for your service.\n  * **Subscription**: Your Azure subscription.\n  * **Location**: Choose a location in the US.\n  * **Pricing tier**: Choose the F0 pricing tier.\n  * **Resource Group**: Choose the existing resource group you used previously.\n  * Read the notice about the use of your data, and select the checkbox.\n7. Wait for the service to be created.\n\n### Create a LUIS App\nTo implement natural language understanding with LUIS, you must first create an app; and then add intents, utterances, and entities to define the commands you want the app to understand.\n1. Open a new browser tab and navigate to https://www.luis.ai/.\n2. Sign in using the Microsoft account associated with your Azure subscription. If this is the first time you have signed into LUIS, you may need to grant the app some permissions to access your account details, and then fill in some information and accept the terms of use.\n3. If a message prompting you to complete a tutorial in which you will create a *Scheduler* app is displayed, close it (you can complete this tutorial later - for now, we'll focus on a simpler example).\n4. Click **New App** and create a new app with the following settings:\n  * **Name**: Simple Home Automation\n  * **Culture**: English\n  * **Description**: A basic home automation example\n5. In the pane on the left, click **Intents**. Then click **Add Prebuilt Domain Intent**, Search for **Home**, Add the following:\n    * **HomeAutomation.TurnOff**\n    * **HomeAutomation.TurnOn**\n6. In the pane on the left, click **Entities**. Then click **Add Prebuilt Domain Entity**, Search for **Home**, Add the following:\n    * **HomeAutomation.Device**\n    * **HomeAutomation.Operation**\n    * **HomeAutomation.Room**\n7. At the top of the page, click **Train** to train the application\n8. After the app has been trained, click **Test**, and then in the test pane, enter the following utterances and verify that they are correctly interpreted as commands for the *HomeAutomation.TurnOn* and *HomeAutomation.TurnOff* intents respectively:\n    * *turn lights on*\n    * *turn off the lights*\n9. IAt the top of the page, click **Publish**. Then click **Publish to production slot**.\n10. After the app has been published, note the **Endpoint** URL that is generated for your app - this includes the location where you provisioned the service, your app ID, and the key assigned to the app.\n\n### Consume the LUIS App\nNow that you have published your LUIS app, you can consume it from a client application by making HTTP requests that include a query string. The query will be used to identify the most likely intent, which will be returned to the calling client as in JSON response.\n\nModify the **endpointUrl** variable declaration in the cell below to reflect the endpoint URL for your app. Then run the cell, and enter a command when prompted to call your service and interpret the command. The JSON response is shown with an appropriate image for each command.\n\nTry the following commands:\n* *Switch on the light*\n* *Turn on the light*\n* *Turn off the light*\n* *Could you switch on the lights on please?*"],"metadata":{}},{"cell_type":"code","source":["import requests\nfrom io import BytesIO\nimport json \n\n# Set up API configuration\nendpointUrl = \"<INSERT LUIS ENDPOINT URL HERE WHICH INCLUDES THE KEY>\"\n\n# Test a command\ncommand = 'Can you switch on the lights please?'\n\n# Call the LUIS service and get the JSON response\nendpoint = endpointUrl + command.replace(\" \",\"+\")\nresponse = requests.get(endpoint)\ndata = json.loads(response.content.decode(\"UTF-8\"))\nprint (data)\n\n# Identify the top scoring intent\nintent = data[\"topScoringIntent\"][\"intent\"]\nprint (\"Found Intent: \" + intent)"],"metadata":{"scrolled":true},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&apos;intents&apos;: [{&apos;intent&apos;: &apos;HomeAutomation.TurnOn&apos;, &apos;score&apos;: 0.9798689}, {&apos;intent&apos;: &apos;None&apos;, &apos;score&apos;: 0.0185729563}, {&apos;intent&apos;: &apos;HomeAutomation.TurnOff&apos;, &apos;score&apos;: 0.009096554}], &apos;entities&apos;: [{&apos;score&apos;: 0.9591603, &apos;entity&apos;: &apos;lights&apos;, &apos;endIndex&apos;: 27, &apos;type&apos;: &apos;HomeAutomation.Device&apos;, &apos;startIndex&apos;: 22}], &apos;query&apos;: &apos;Can you switch on the lights please?&apos;, &apos;topScoringIntent&apos;: {&apos;intent&apos;: &apos;HomeAutomation.TurnOn&apos;, &apos;score&apos;: 0.9798689}}\nFound Intent: HomeAutomation.TurnOn\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["## Combining Speech Recognition and Natural Language Understanding\nAn obvious next step is to combine speech recognition with natural language understanding so that a spoken command can be interpreted and the appropriate action taken.\n\n### Download Command Audio\nLet's start by downloading and playing some spoken commands for our home automation system. <br>\nhttps://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOn.wav <br>\nhttps://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOff.wav <br>\nRight-click on above links and \"Save Link As..\" to download the audio files and listen to them"],"metadata":{}},{"cell_type":"code","source":["%sh\n# Get the \"lights on\" command\ncurl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOn.wav -o /dbfs/FileStore/media/LightOn.wav\ncurl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOff.wav -o /dbfs/FileStore/media/LightOff.wav\nls -lt /dbfs/FileStore/media"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0  535k    0  1934    0     0   3806      0  0:02:24 --:--:--  0:02:24  3799\n100  535k  100  535k    0     0   828k      0 --:--:-- --:--:-- --:--:--  828k\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100  526k  100  526k    0     0  1147k      0 --:--:-- --:--:-- --:--:-- 1150k\ntotal 0\n-rw-r--r-- 1 root root 539452 Sep  4 18:53 LightOff.wav\n-rw-r--r-- 1 root root 548368 Sep  4 18:53 LightOn.wav\n-rw-r--r-- 1 root root 663648 Sep  4 18:45 RainSpain.wav\n</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["### Transcribe and Interpret the \"Light On\" Command\nNow you can use the Bing Speech API to transcribe the command, and then use your LUIS app to interpret the command and take the appropriate action.\n\nYou should have already installed the **SpeechRecognition** package, set the **speechKey** for your Bing Speech service, and set the **apiUrl**, **appId**, and **appKey** for your LUIS app (if you have closed and re-opened the notebook, re-run the appropriate cells above to set these).\n\nRun the cell below that to test the \"Light On\" command."],"metadata":{}},{"cell_type":"code","source":["import speech_recognition as sr\nfrom matplotlib.pyplot import imshow\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport json \n\n# Specify which audio file to use\naudioFile = \"/dbfs/FileStore/media/LightOff.wav\"\n\n# Read the audio file\nr = sr.Recognizer()\nwith sr.AudioFile(audioFile) as source:\n    audio = r.record(source)\n\ntry:\n    # transcribe speech using the Bing Speech API\n    transcription = r.recognize_bing(audio, key=speechKey)\n    \n    # Call the LUIS service and get the JSON response\n    endpoint = endpointUrl + transcription.replace(\" \",\"+\")\n    response = requests.get(endpoint)\n    data = json.loads(response.content.decode(\"UTF-8\"))\n\n    # Identify the top scoring intent\n    intent = data[\"topScoringIntent\"][\"intent\"]\n    print(data)\n    print(intent)\n    \nexcept sr.UnknownValueError:\n    print(\"Bing Speech could not understand audio\")\nexcept sr.RequestError as e:\n    print (e)\n    print(\"Could not request results from the Bing Speech service; {0}\".format(e))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&apos;intents&apos;: [{&apos;intent&apos;: &apos;HomeAutomation.TurnOff&apos;, &apos;score&apos;: 0.989046931}, {&apos;intent&apos;: &apos;HomeAutomation.TurnOn&apos;, &apos;score&apos;: 0.016409345}, {&apos;intent&apos;: &apos;None&apos;, &apos;score&apos;: 0.009934014}], &apos;entities&apos;: [{&apos;score&apos;: 0.8938316, &apos;entity&apos;: &apos;light&apos;, &apos;endIndex&apos;: 13, &apos;type&apos;: &apos;HomeAutomation.Device&apos;, &apos;startIndex&apos;: 9}], &apos;query&apos;: &apos;Turn the light off.&apos;, &apos;topScoringIntent&apos;: {&apos;intent&apos;: &apos;HomeAutomation.TurnOff&apos;, &apos;score&apos;: 0.989046931}}\nHomeAutomation.TurnOff\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["### Transcribe and Interpret the \"Light Off\" Command\nModify the cell above to set the **audioFile** variable to the **LightOff.wav** audio file, and then run the cell again to test it"],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python [default]","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":"3"},"version":"3.6.4","nbconvert_exporter":"python","file_extension":".py"},"name":"Azure Speech-Text-LUIS - Home Automation","notebookId":2774167366548128},"nbformat":4,"nbformat_minor":0}